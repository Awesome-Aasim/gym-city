{"episode_reward": [8.0, 21.0, 25.0, 16.0, 8.0, 22.0, 14.0, 18.0, 12.0, 9.0, 6.0, 7.0, 32.0, 2.0], "nb_steps": [1001, 2002, 3003, 4004, 5005, 6006, 7007, 8008, 9009, 10010, 11011, 12012, 13013, 14014], "mean_absolute_error": [NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN], "loss": [NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN], "mean_eps": [NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN], "nb_episode_steps": [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], "duration": [3.012645959854126, 2.62626576423645, 2.5905990600585938, 2.5176501274108887, 2.5280699729919434, 2.6900291442871094, 2.691171884536743, 2.5403189659118652, 2.4804019927978516, 2.6510050296783447, 2.6126928329467773, 2.697967052459717, 2.438155174255371, 1.9533841609954834], "episode": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13], "mean_q": [NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN]}